{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853683b0-8e35-4367-bc88-52d5a7eee191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60e87cd-91b6-4f3c-a7a0-d029a9d2812c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236</td>\n",
       "      <td>488</td>\n",
       "      <td>16</td>\n",
       "      <td>221</td>\n",
       "      <td>382</td>\n",
       "      <td>97</td>\n",
       "      <td>-4.472136</td>\n",
       "      <td>0.107472</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340874</td>\n",
       "      <td>0.870542</td>\n",
       "      <td>1.962937</td>\n",
       "      <td>7.466666</td>\n",
       "      <td>11.547794</td>\n",
       "      <td>8.822916</td>\n",
       "      <td>9.046424</td>\n",
       "      <td>7.895535</td>\n",
       "      <td>11.010677</td>\n",
       "      <td>20.107472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>206</td>\n",
       "      <td>357</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>7.810250</td>\n",
       "      <td>0.763713</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>...</td>\n",
       "      <td>12.484882</td>\n",
       "      <td>7.168680</td>\n",
       "      <td>2.885415</td>\n",
       "      <td>12.413973</td>\n",
       "      <td>10.260494</td>\n",
       "      <td>10.091351</td>\n",
       "      <td>9.270888</td>\n",
       "      <td>3.173994</td>\n",
       "      <td>13.921871</td>\n",
       "      <td>61.763713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>429</td>\n",
       "      <td>49</td>\n",
       "      <td>481</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>146</td>\n",
       "      <td>8.602325</td>\n",
       "      <td>0.651162</td>\n",
       "      <td>1</td>\n",
       "      <td>430</td>\n",
       "      <td>...</td>\n",
       "      <td>14.030257</td>\n",
       "      <td>0.394970</td>\n",
       "      <td>8.160625</td>\n",
       "      <td>12.592059</td>\n",
       "      <td>8.937577</td>\n",
       "      <td>2.265191</td>\n",
       "      <td>11.255721</td>\n",
       "      <td>12.794841</td>\n",
       "      <td>12.080951</td>\n",
       "      <td>74.651162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414</td>\n",
       "      <td>350</td>\n",
       "      <td>481</td>\n",
       "      <td>370</td>\n",
       "      <td>208</td>\n",
       "      <td>158</td>\n",
       "      <td>8.306624</td>\n",
       "      <td>0.424645</td>\n",
       "      <td>1</td>\n",
       "      <td>340</td>\n",
       "      <td>...</td>\n",
       "      <td>2.789577</td>\n",
       "      <td>6.416708</td>\n",
       "      <td>10.549814</td>\n",
       "      <td>11.456437</td>\n",
       "      <td>6.468099</td>\n",
       "      <td>2.519049</td>\n",
       "      <td>0.258284</td>\n",
       "      <td>9.317696</td>\n",
       "      <td>5.383098</td>\n",
       "      <td>69.424645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>318</td>\n",
       "      <td>359</td>\n",
       "      <td>20</td>\n",
       "      <td>218</td>\n",
       "      <td>317</td>\n",
       "      <td>301</td>\n",
       "      <td>8.124038</td>\n",
       "      <td>0.767304</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886560</td>\n",
       "      <td>1.919999</td>\n",
       "      <td>2.268203</td>\n",
       "      <td>0.149421</td>\n",
       "      <td>4.105907</td>\n",
       "      <td>10.416291</td>\n",
       "      <td>6.816217</td>\n",
       "      <td>8.586960</td>\n",
       "      <td>4.512419</td>\n",
       "      <td>66.767304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89995</th>\n",
       "      <td>328</td>\n",
       "      <td>44</td>\n",
       "      <td>320</td>\n",
       "      <td>364</td>\n",
       "      <td>7</td>\n",
       "      <td>73</td>\n",
       "      <td>4.898979</td>\n",
       "      <td>0.563878</td>\n",
       "      <td>1</td>\n",
       "      <td>315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266362</td>\n",
       "      <td>6.185887</td>\n",
       "      <td>7.716837</td>\n",
       "      <td>10.144664</td>\n",
       "      <td>1.711649</td>\n",
       "      <td>3.849704</td>\n",
       "      <td>12.401903</td>\n",
       "      <td>14.195540</td>\n",
       "      <td>2.371207</td>\n",
       "      <td>24.563878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89996</th>\n",
       "      <td>217</td>\n",
       "      <td>472</td>\n",
       "      <td>174</td>\n",
       "      <td>327</td>\n",
       "      <td>255</td>\n",
       "      <td>389</td>\n",
       "      <td>-3.605551</td>\n",
       "      <td>0.861690</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>5.147059</td>\n",
       "      <td>4.209660</td>\n",
       "      <td>14.482770</td>\n",
       "      <td>1.375031</td>\n",
       "      <td>6.386263</td>\n",
       "      <td>10.107582</td>\n",
       "      <td>12.637902</td>\n",
       "      <td>6.576331</td>\n",
       "      <td>6.863238</td>\n",
       "      <td>13.861690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89997</th>\n",
       "      <td>189</td>\n",
       "      <td>406</td>\n",
       "      <td>213</td>\n",
       "      <td>57</td>\n",
       "      <td>494</td>\n",
       "      <td>190</td>\n",
       "      <td>6.164414</td>\n",
       "      <td>0.633704</td>\n",
       "      <td>1</td>\n",
       "      <td>221</td>\n",
       "      <td>...</td>\n",
       "      <td>14.902807</td>\n",
       "      <td>2.886534</td>\n",
       "      <td>8.683168</td>\n",
       "      <td>4.522339</td>\n",
       "      <td>1.556421</td>\n",
       "      <td>10.971184</td>\n",
       "      <td>7.792226</td>\n",
       "      <td>8.422623</td>\n",
       "      <td>3.041409</td>\n",
       "      <td>38.633704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89998</th>\n",
       "      <td>418</td>\n",
       "      <td>478</td>\n",
       "      <td>163</td>\n",
       "      <td>35</td>\n",
       "      <td>390</td>\n",
       "      <td>77</td>\n",
       "      <td>-3.605551</td>\n",
       "      <td>0.687309</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>...</td>\n",
       "      <td>12.743029</td>\n",
       "      <td>2.525611</td>\n",
       "      <td>11.050145</td>\n",
       "      <td>6.589943</td>\n",
       "      <td>12.622192</td>\n",
       "      <td>10.596839</td>\n",
       "      <td>0.647584</td>\n",
       "      <td>8.746364</td>\n",
       "      <td>1.246682</td>\n",
       "      <td>13.687309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89999</th>\n",
       "      <td>244</td>\n",
       "      <td>335</td>\n",
       "      <td>337</td>\n",
       "      <td>152</td>\n",
       "      <td>386</td>\n",
       "      <td>202</td>\n",
       "      <td>7.681146</td>\n",
       "      <td>0.613207</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>11.069077</td>\n",
       "      <td>6.303448</td>\n",
       "      <td>14.031393</td>\n",
       "      <td>1.877340</td>\n",
       "      <td>13.361607</td>\n",
       "      <td>2.164695</td>\n",
       "      <td>11.255181</td>\n",
       "      <td>3.404303</td>\n",
       "      <td>9.587379</td>\n",
       "      <td>59.613207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90000 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5         6         7  8    9  ...  \\\n",
       "0      236  488   16  221  382   97 -4.472136  0.107472  0  132  ...   \n",
       "1      386  206  357  232    1  198  7.810250  0.763713  1  143  ...   \n",
       "2      429   49  481  111  111  146  8.602325  0.651162  1  430  ...   \n",
       "3      414  350  481  370  208  158  8.306624  0.424645  1  340  ...   \n",
       "4      318  359   20  218  317  301  8.124038  0.767304  1  212  ...   \n",
       "...    ...  ...  ...  ...  ...  ...       ...       ... ..  ...  ...   \n",
       "89995  328   44  320  364    7   73  4.898979  0.563878  1  315  ...   \n",
       "89996  217  472  174  327  255  389 -3.605551  0.861690  0  144  ...   \n",
       "89997  189  406  213   57  494  190  6.164414  0.633704  1  221  ...   \n",
       "89998  418  478  163   35  390   77 -3.605551  0.687309  0  289  ...   \n",
       "89999  244  335  337  152  386  202  7.681146  0.613207  1   26  ...   \n",
       "\n",
       "              44        45         46         47         48         49  \\\n",
       "0      13.340874  0.870542   1.962937   7.466666  11.547794   8.822916   \n",
       "1      12.484882  7.168680   2.885415  12.413973  10.260494  10.091351   \n",
       "2      14.030257  0.394970   8.160625  12.592059   8.937577   2.265191   \n",
       "3       2.789577  6.416708  10.549814  11.456437   6.468099   2.519049   \n",
       "4       1.886560  1.919999   2.268203   0.149421   4.105907  10.416291   \n",
       "...          ...       ...        ...        ...        ...        ...   \n",
       "89995   0.266362  6.185887   7.716837  10.144664   1.711649   3.849704   \n",
       "89996   5.147059  4.209660  14.482770   1.375031   6.386263  10.107582   \n",
       "89997  14.902807  2.886534   8.683168   4.522339   1.556421  10.971184   \n",
       "89998  12.743029  2.525611  11.050145   6.589943  12.622192  10.596839   \n",
       "89999  11.069077  6.303448  14.031393   1.877340  13.361607   2.164695   \n",
       "\n",
       "              50         51         52     target  \n",
       "0       9.046424   7.895535  11.010677  20.107472  \n",
       "1       9.270888   3.173994  13.921871  61.763713  \n",
       "2      11.255721  12.794841  12.080951  74.651162  \n",
       "3       0.258284   9.317696   5.383098  69.424645  \n",
       "4       6.816217   8.586960   4.512419  66.767304  \n",
       "...          ...        ...        ...        ...  \n",
       "89995  12.401903  14.195540   2.371207  24.563878  \n",
       "89996  12.637902   6.576331   6.863238  13.861690  \n",
       "89997   7.792226   8.422623   3.041409  38.633704  \n",
       "89998   0.647584   8.746364   1.246682  13.687309  \n",
       "89999  11.255181   3.404303   9.587379  59.613207  \n",
       "\n",
       "[90000 rows x 54 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('internship_train.csv')\n",
    "targets = df['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f4ccec-102a-4cac-a7c5-f4c679f3ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns='target'), \n",
    "    targets, \n",
    "    test_size=0.001, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2c3944-3275-4890-a6c1-520c3e6f9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='regression.h5', monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'min'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
    "                                   patience=3, \n",
    "                                   verbose=1, mode='min', min_delta=0.0001, cooldown=2, min_lr=1e-6)   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a679c698-7e36-4193-954e-452b9fe0182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "\n",
    "\n",
    "normalizer = layers.Normalization(input_shape=[53,], axis=None)\n",
    "\n",
    "def build_and_compile_model(norm):\n",
    "    model = keras.Sequential([\n",
    "      norm,\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(32, activation='relu'),\n",
    "      layers.Dense(16, activation='relu'),\n",
    "      layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001), metrics = ['RootMeanSquaredError'] )\n",
    "    return model\n",
    "\n",
    "model = build_and_compile_model(normalizer)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c61258-7548-4555-9fc3-086a8eb66886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1124/1124 [==============================] - 3s 2ms/step - loss: 25.9244 - root_mean_squared_error: 30.5024 - val_loss: 25.5878 - val_root_mean_squared_error: 29.9231\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 25.58778, saving model to model.h5\n",
      "Epoch 2/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 25.4058 - root_mean_squared_error: 29.5860 - val_loss: 25.2239 - val_root_mean_squared_error: 29.2482\n",
      "\n",
      "Epoch 00002: val_loss improved from 25.58778 to 25.22392, saving model to model.h5\n",
      "Epoch 3/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 25.3044 - root_mean_squared_error: 29.4091 - val_loss: 25.4629 - val_root_mean_squared_error: 29.5678\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 25.22392\n",
      "Epoch 4/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 25.2741 - root_mean_squared_error: 29.3546 - val_loss: 25.5037 - val_root_mean_squared_error: 29.6473\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 25.22392\n",
      "Epoch 5/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 25.0532 - root_mean_squared_error: 29.1306 - val_loss: 20.9680 - val_root_mean_squared_error: 24.5911\n",
      "\n",
      "Epoch 00005: val_loss improved from 25.22392 to 20.96798, saving model to model.h5\n",
      "Epoch 6/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 6.5953 - root_mean_squared_error: 9.6955 - val_loss: 3.6899 - val_root_mean_squared_error: 4.7969\n",
      "\n",
      "Epoch 00006: val_loss improved from 20.96798 to 3.68992, saving model to model.h5\n",
      "Epoch 7/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 3.7252 - root_mean_squared_error: 4.8693 - val_loss: 3.4050 - val_root_mean_squared_error: 4.4251\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.68992 to 3.40495, saving model to model.h5\n",
      "Epoch 8/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 3.4967 - root_mean_squared_error: 4.6025 - val_loss: 3.1908 - val_root_mean_squared_error: 4.1699\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.40495 to 3.19077, saving model to model.h5\n",
      "Epoch 9/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 3.3658 - root_mean_squared_error: 4.4404 - val_loss: 2.8645 - val_root_mean_squared_error: 3.8159\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.19077 to 2.86452, saving model to model.h5\n",
      "Epoch 10/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 3.2422 - root_mean_squared_error: 4.3018 - val_loss: 3.8592 - val_root_mean_squared_error: 4.9007\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.86452\n",
      "Epoch 11/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 3.0414 - root_mean_squared_error: 4.0431 - val_loss: 2.8555 - val_root_mean_squared_error: 3.8371\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.86452 to 2.85555, saving model to model.h5\n",
      "Epoch 12/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 3.0005 - root_mean_squared_error: 3.9895 - val_loss: 3.4754 - val_root_mean_squared_error: 4.4747\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.85555\n",
      "Epoch 13/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.9222 - root_mean_squared_error: 3.8953 - val_loss: 3.0013 - val_root_mean_squared_error: 3.9456\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.85555\n",
      "Epoch 14/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.9354 - root_mean_squared_error: 3.9155 - val_loss: 2.5120 - val_root_mean_squared_error: 3.3876\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.85555 to 2.51199, saving model to model.h5\n",
      "Epoch 15/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.8283 - root_mean_squared_error: 3.7862 - val_loss: 3.0041 - val_root_mean_squared_error: 3.9557\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.51199\n",
      "Epoch 16/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.7488 - root_mean_squared_error: 3.6713 - val_loss: 2.3860 - val_root_mean_squared_error: 3.2421\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.51199 to 2.38602, saving model to model.h5\n",
      "Epoch 17/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.7109 - root_mean_squared_error: 3.6178 - val_loss: 2.4076 - val_root_mean_squared_error: 3.2940\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.38602\n",
      "Epoch 18/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.6332 - root_mean_squared_error: 3.5314 - val_loss: 4.2961 - val_root_mean_squared_error: 5.2404\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.38602\n",
      "Epoch 19/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.5157 - root_mean_squared_error: 3.3846 - val_loss: 2.4142 - val_root_mean_squared_error: 3.2322\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.38602\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 20/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.2422 - root_mean_squared_error: 3.0443 - val_loss: 2.5913 - val_root_mean_squared_error: 3.4746\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.38602\n",
      "Epoch 21/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.2844 - root_mean_squared_error: 3.0925 - val_loss: 2.3066 - val_root_mean_squared_error: 3.1120\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.38602 to 2.30656, saving model to model.h5\n",
      "Epoch 22/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.1958 - root_mean_squared_error: 2.9852 - val_loss: 2.3287 - val_root_mean_squared_error: 3.1004\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.30656\n",
      "Epoch 23/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.1555 - root_mean_squared_error: 2.9310 - val_loss: 1.9982 - val_root_mean_squared_error: 2.7493\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.30656 to 1.99822, saving model to model.h5\n",
      "Epoch 24/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.1625 - root_mean_squared_error: 2.9363 - val_loss: 2.0440 - val_root_mean_squared_error: 2.7781\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.99822\n",
      "Epoch 25/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.1342 - root_mean_squared_error: 2.8975 - val_loss: 2.1806 - val_root_mean_squared_error: 2.9213\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.99822\n",
      "Epoch 26/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 2.0716 - root_mean_squared_error: 2.8148 - val_loss: 2.0425 - val_root_mean_squared_error: 2.8063\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.99822\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 27/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.9129 - root_mean_squared_error: 2.6249 - val_loss: 1.8537 - val_root_mean_squared_error: 2.5595\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.99822 to 1.85374, saving model to model.h5\n",
      "Epoch 28/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.9083 - root_mean_squared_error: 2.6155 - val_loss: 1.9750 - val_root_mean_squared_error: 2.6783\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.85374\n",
      "Epoch 29/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.8971 - root_mean_squared_error: 2.6027 - val_loss: 1.8973 - val_root_mean_squared_error: 2.5989\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.85374\n",
      "Epoch 30/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.8756 - root_mean_squared_error: 2.5776 - val_loss: 1.8189 - val_root_mean_squared_error: 2.5116\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.85374 to 1.81892, saving model to model.h5\n",
      "Epoch 31/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.8486 - root_mean_squared_error: 2.5412 - val_loss: 1.8728 - val_root_mean_squared_error: 2.5412\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.81892\n",
      "Epoch 32/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.8376 - root_mean_squared_error: 2.5253 - val_loss: 1.7820 - val_root_mean_squared_error: 2.4627\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.81892 to 1.78199, saving model to model.h5\n",
      "Epoch 33/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.8249 - root_mean_squared_error: 2.5066 - val_loss: 2.3294 - val_root_mean_squared_error: 3.0726\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.78199\n",
      "Epoch 34/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.8034 - root_mean_squared_error: 2.4776 - val_loss: 1.7523 - val_root_mean_squared_error: 2.4229\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.78199 to 1.75234, saving model to model.h5\n",
      "Epoch 35/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.7771 - root_mean_squared_error: 2.4448 - val_loss: 1.9029 - val_root_mean_squared_error: 2.5917\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.75234\n",
      "Epoch 36/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.7652 - root_mean_squared_error: 2.4247 - val_loss: 1.6947 - val_root_mean_squared_error: 2.3449\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.75234 to 1.69465, saving model to model.h5\n",
      "Epoch 37/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.7553 - root_mean_squared_error: 2.4158 - val_loss: 1.9924 - val_root_mean_squared_error: 2.6682\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.69465\n",
      "Epoch 38/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.7371 - root_mean_squared_error: 2.3897 - val_loss: 1.7576 - val_root_mean_squared_error: 2.4007\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.69465\n",
      "Epoch 39/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.7322 - root_mean_squared_error: 2.3755 - val_loss: 1.7155 - val_root_mean_squared_error: 2.3519\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.69465\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 40/40\n",
      "1124/1124 [==============================] - 2s 2ms/step - loss: 1.6263 - root_mean_squared_error: 2.2574 - val_loss: 1.6828 - val_root_mean_squared_error: 2.3219\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.69465 to 1.68283, saving model to model.h5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64,validation_split = 0.2, epochs=40, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c4979d-3183-458b-bc1e-3fc11e75f00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_root_mean_squared_error</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.765221</td>\n",
       "      <td>2.424739</td>\n",
       "      <td>1.694651</td>\n",
       "      <td>2.344879</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.755332</td>\n",
       "      <td>2.415820</td>\n",
       "      <td>1.992387</td>\n",
       "      <td>2.668187</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.737072</td>\n",
       "      <td>2.389747</td>\n",
       "      <td>1.757598</td>\n",
       "      <td>2.400681</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.732239</td>\n",
       "      <td>2.375457</td>\n",
       "      <td>1.715544</td>\n",
       "      <td>2.351883</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.626335</td>\n",
       "      <td>2.257421</td>\n",
       "      <td>1.682828</td>\n",
       "      <td>2.321895</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  root_mean_squared_error  val_loss  val_root_mean_squared_error  \\\n",
       "35  1.765221                 2.424739  1.694651                     2.344879   \n",
       "36  1.755332                 2.415820  1.992387                     2.668187   \n",
       "37  1.737072                 2.389747  1.757598                     2.400681   \n",
       "38  1.732239                 2.375457  1.715544                     2.351883   \n",
       "39  1.626335                 2.257421  1.682828                     2.321895   \n",
       "\n",
       "          lr  epoch  \n",
       "35  0.000250     35  \n",
       "36  0.000250     36  \n",
       "37  0.000250     37  \n",
       "38  0.000250     38  \n",
       "39  0.000125     39  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420bea9b-2b3b-466d-8b17-0bfcdb5cf6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.ylim([0, 30])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d4a33e-9bb1-4ca3-9342-4abe982f253b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f59bdd-0b92-452e-ad4b-c6c7b211aa62",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ecf8f88-33c2-4456-85dd-e6796fd657e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.755246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.959122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.276759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.865379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62.086823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>47.675591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>32.414650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>22.716415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>15.649108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>90.283043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          preds\n",
       "0      9.755246\n",
       "1     79.959122\n",
       "2      8.276759\n",
       "3     38.865379\n",
       "4     62.086823\n",
       "...         ...\n",
       "9995  47.675591\n",
       "9996  32.414650\n",
       "9997  22.716415\n",
       "9998  15.649108\n",
       "9999  90.283043\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('internship_hidden_test.csv')\n",
    "preds = model.predict(test)\n",
    "preds_df = pd.DataFrame(preds, columns = ['preds'])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a92ccde2-04ba-4bd2-9e58-71e4cfa323cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df.to_csv('preds_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
